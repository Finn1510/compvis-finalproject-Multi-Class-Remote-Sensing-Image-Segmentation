{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "973e6747",
   "metadata": {},
   "source": [
    "# DDCM-Net: Dense Dilated Convolutions Merging Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebb804a",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43a8ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from model import create_model, create_trainer\n",
    "from dataset_loader import create_dataloaders\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Check device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f03339",
   "metadata": {},
   "source": [
    "## 2. Model Architecture Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ed5e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "NUM_CLASSES = 6\n",
    "BACKBONE = 'resnet50'\n",
    "PRETRAINED = True\n",
    "\n",
    "# Class names for visualization\n",
    "CLASS_NAMES = [\n",
    "    'Impervious surfaces',  # 0 - White\n",
    "    'Building',             # 1 - Blue  \n",
    "    'Low vegetation',       # 2 - Cyan\n",
    "    'Tree',                 # 3 - Green\n",
    "    'Car',                  # 4 - Yellow\n",
    "    'Clutter/background'    # 5 - Red\n",
    "]\n",
    "\n",
    "# Create model\n",
    "model = create_model(\n",
    "    variant='base',\n",
    "    num_classes=NUM_CLASSES,\n",
    "    backbone=BACKBONE,\n",
    "    pretrained=PRETRAINED\n",
    ")\n",
    "\n",
    "# Create trainer wrapper\n",
    "trainer = create_trainer(\n",
    "    model=model,\n",
    "    device=device,\n",
    "    class_names=CLASS_NAMES\n",
    ")\n",
    "\n",
    "# Print model information\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Architecture: DDCM-Net with {BACKBONE} backbone\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bc6efb",
   "metadata": {},
   "source": [
    "### Load model from a previous training session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4786af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer = trainer.load_model(\"best_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc7bd78",
   "metadata": {},
   "source": [
    "## 3. Dataset Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8dd4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset configuration\n",
    "DATA_ROOT = \"./data\"\n",
    "DATASET = \"potsdam\"  # or \"vaihingen\" or \"both\"\n",
    "BATCH_SIZE = 5  \n",
    "NUM_WORKERS = 4\n",
    "TRAIN_PATCH_SIZE = 256  \n",
    "VAL_PATCH_SIZE = 448   \n",
    "\n",
    "print(\"Loading dataset...\")\n",
    "print(f\"  Training patches: {TRAIN_PATCH_SIZE}x{TRAIN_PATCH_SIZE}\")\n",
    "print(f\"  Validation patches: {VAL_PATCH_SIZE}x{VAL_PATCH_SIZE}\")\n",
    "\n",
    "# Check if data exists\n",
    "if not os.path.exists(DATA_ROOT):\n",
    "    print(f\"Data directory not found: {DATA_ROOT}\")\n",
    "    print(\"Please ensure you have the ISPRS dataset in the data folder\")\n",
    "    print(\"Expected structure:\")\n",
    "    print(\"data/\")\n",
    "    print(\"├── potsdam/\")\n",
    "    print(\"│   ├── images/\")\n",
    "    print(\"│   └── labels/\")\n",
    "    print(\"└── vaihingen/\")\n",
    "    print(\"    ├── images/\")\n",
    "    print(\"    └── labels/\")\n",
    "else:\n",
    "    train_loader, val_loader, test_loader, holdout_loader = create_dataloaders(\n",
    "        root_dir=DATA_ROOT,\n",
    "        dataset=DATASET,\n",
    "        train_patch_size=TRAIN_PATCH_SIZE,\n",
    "        val_patch_size=VAL_PATCH_SIZE,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        num_workers=NUM_WORKERS\n",
    "    )\n",
    "    \n",
    "    print(f\"Dataset: {DATASET.upper()}\")\n",
    "    print(f\"Batch size: {BATCH_SIZE}\")\n",
    "    print(f\"Train patch size: {TRAIN_PATCH_SIZE}x{TRAIN_PATCH_SIZE}\")\n",
    "    print(f\"Val/Test patch size: {VAL_PATCH_SIZE}x{VAL_PATCH_SIZE}\")\n",
    "    print(f\"Train batches: {len(train_loader):,}\")\n",
    "    print(f\"Validation batches: {len(val_loader):,}\")\n",
    "    print(f\"Test batches: {len(test_loader):,}\")\n",
    "    \n",
    "    # Show a sample batch\n",
    "    # sample_images, sample_labels = next(iter(train_loader))\n",
    "    # print(f\"Sample batch shape - Images: {sample_images.shape}, Labels: {sample_labels.shape}\")\n",
    "    # print(f\"Classes in sample: {torch.unique(sample_labels).tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ef9dea",
   "metadata": {},
   "source": [
    "### Dataset Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ef7ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize dataset samples\n",
    "def visualize_dataset_samples(dataloader, num_samples=4):\n",
    "    \"\"\"Visualize samples from the dataset\"\"\"\n",
    "    fig, axes = plt.subplots(2, num_samples, figsize=(20, 10))\n",
    "    \n",
    "    # Get a batch\n",
    "    images, labels = next(iter(dataloader))\n",
    "    \n",
    "    for i in range(min(num_samples, len(images))):\n",
    "        # Get single sample\n",
    "        img = images[i]\n",
    "        label = labels[i]\n",
    "        \n",
    "        # Denormalize image for visualization\n",
    "        mean = torch.tensor([0.485, 0.456, 0.406])\n",
    "        std = torch.tensor([0.229, 0.224, 0.225])\n",
    "        img_denorm = img * std[:, None, None] + mean[:, None, None]\n",
    "        img_denorm = torch.clamp(img_denorm, 0, 1)\n",
    "        \n",
    "        # Plot image\n",
    "        axes[0, i].imshow(img_denorm.permute(1, 2, 0))\n",
    "        axes[0, i].set_title(f'Image {i+1}')\n",
    "        axes[0, i].axis('off')\n",
    "        \n",
    "        # Plot label\n",
    "        im = axes[1, i].imshow(label, cmap='tab10', vmin=0, vmax=5)\n",
    "        axes[1, i].set_title(f'Ground Truth {i+1}')\n",
    "        axes[1, i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print class statistics\n",
    "    unique_classes = torch.unique(labels)\n",
    "    print(f\"Classes in this batch: {unique_classes.tolist()}\")\n",
    "    for class_id in unique_classes:\n",
    "        class_name = CLASS_NAMES[class_id]\n",
    "        pixel_count = (labels == class_id).sum().item()\n",
    "        percentage = pixel_count / labels.numel() * 100\n",
    "        print(f\"  {class_id}: {class_name} - {pixel_count:,} pixels ({percentage:.1f}%)\")\n",
    "\n",
    "# Visualize samples if data is available\n",
    "if 'train_loader' in locals():\n",
    "    print(\"Dataset samples:\")\n",
    "    visualize_dataset_samples(train_loader, num_samples=4)\n",
    "else:\n",
    "    print(\"Dataset not loaded. Please ensure data is available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca978b5",
   "metadata": {},
   "source": [
    "## 4. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7a37ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "EPOCHS = 100\n",
    "LEARNING_RATE = 8.5e-5 / np.sqrt(2)  \n",
    "WEIGHT_DECAY = 2e-5\n",
    "USE_DUAL_LR = True  \n",
    "\n",
    "print(\"Starting model training...\")\n",
    "print(f\"Configuration:\")\n",
    "print(f\"Epochs: {EPOCHS}\")\n",
    "print(f\"Learning rate: {LEARNING_RATE:.2e}\")\n",
    "print(f\"Weight decay: {WEIGHT_DECAY}\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# Start training if data is available\n",
    "if 'train_loader' in locals() and 'val_loader' in locals():\n",
    "    print(f\"\\nTraining started...\")\n",
    "        \n",
    "    # Train the model\n",
    "    history = trainer.fit(\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        epochs=EPOCHS,\n",
    "        lr=LEARNING_RATE,\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "        use_mfb=True,\n",
    "        lr_scheduler='step',  # Will be overridden by use_dual_lr\n",
    "        use_dual_lr=USE_DUAL_LR  \n",
    "    )\n",
    "    \n",
    "else:\n",
    "    print(\"Cannot start training - dataset not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5e3778",
   "metadata": {},
   "source": [
    "## 5. Training Visualization and Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576a6d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "if 'history' in locals() and trainer.history['train_loss']:\n",
    "    print(\"Training History Visualization\")\n",
    "    \n",
    "    # Plot using trainer's built-in visualization\n",
    "    trainer.plot_training_history(figsize=(18, 6))\n",
    "    \n",
    "    # Print final metrics\n",
    "    final_train_loss = trainer.history['train_loss'][-1]\n",
    "    final_val_loss = trainer.history['val_loss'][-1]\n",
    "    final_train_acc = trainer.history['train_acc'][-1]\n",
    "    final_val_acc = trainer.history['val_acc'][-1]\n",
    "    final_train_miou = trainer.history['train_miou'][-1]\n",
    "    final_val_miou = trainer.history['val_miou'][-1]\n",
    "    \n",
    "    print(\"\\nFinal Training Metrics:\")\n",
    "    print(f\"  Train Loss: {final_train_loss:.4f} | Val Loss: {final_val_loss:.4f}\")\n",
    "    print(f\"  Train Acc:  {final_train_acc:.3f} | Val Acc:  {final_val_acc:.3f}\")\n",
    "    print(f\"  Train mIoU: {final_train_miou:.3f} | Val mIoU: {final_val_miou:.3f}\")\n",
    "    \n",
    "    # Find best epoch\n",
    "    best_epoch = np.argmax(trainer.history['val_miou']) + 1\n",
    "    best_miou = max(trainer.history['val_miou'])\n",
    "    print(f\"\\nBest Validation mIoU: {best_miou:.3f} (Epoch {best_epoch})\")\n",
    "    \n",
    "else:\n",
    "    print(\"No training history available.\")\n",
    "    print(\"Please run training first or load a pre-trained model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617474b9",
   "metadata": {},
   "source": [
    "## 5.5. Continue Training from Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64816604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for continued training\n",
    "CONTINUE_TRAINING = True  # Set to True to enable\n",
    "ADDITIONAL_EPOCHS = 20     \n",
    "LAST_COMPLETED_EPOCH = 1 \n",
    "LEARNING_RATE = 1e-6 \n",
    "WEIGHT_DECAY = 2e-5\n",
    "\n",
    "if CONTINUE_TRAINING:\n",
    "    if 'trainer' in locals() and 'train_loader' in locals() and 'val_loader' in locals():\n",
    "        # Load the saved model\n",
    "        if os.path.exists('best_model.pth'):\n",
    "            print(\"Loading best model for continued training...\")\n",
    "            trainer.load_model('best_model.pth')\n",
    "            \n",
    "            # Show current training state\n",
    "            total_epochs_trained = len(trainer.history['train_loss'])\n",
    "            print(f\"Model has been trained for {total_epochs_trained} epochs\")\n",
    "            \n",
    "            if trainer.history['val_miou']:\n",
    "                current_best = max(trainer.history['val_miou'])\n",
    "                print(f\"Current best validation mIoU: {current_best:.3f}\")\n",
    "            \n",
    "            print(f\"\\nContinuing training for {ADDITIONAL_EPOCHS} more epochs...\")\n",
    "            updated_history = trainer.continue_training(\n",
    "                train_loader=train_loader, \n",
    "                val_loader=val_loader,\n",
    "                additional_epochs=ADDITIONAL_EPOCHS,\n",
    "                current_epoch=LAST_COMPLETED_EPOCH,  # Use this to set the LR correctly\n",
    "                initial_lr=LEARNING_RATE,\n",
    "                weight_decay=WEIGHT_DECAY\n",
    "            )\n",
    "            \n",
    "            # Plot updated training history\n",
    "            print(\"\\nUpdated Training History:\")\n",
    "            trainer.plot_training_history(figsize=(18, 6))\n",
    "            \n",
    "        else:\n",
    "            print(\"No saved model found at 'best_model.pth'!\")\n",
    "            print(\"Please train the model first or check the model path.\")\n",
    "    else:\n",
    "        print(\"Dataloaders not found!\")\n",
    "        print(\"Please ensure 'trainer', 'train_loader', and 'val_loader' are available.\")\n",
    "else:\n",
    "    print(\"Continued training is disabled.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420d790a",
   "metadata": {},
   "source": [
    "## 6. Model Prediction and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88182652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model for inference\n",
    "if os.path.exists('best_model.pth'):\n",
    "    print(\"Loading best trained model...\")\n",
    "    trainer.load_model('best_model.pth')\n",
    "    print(\"Model loaded successfully!\")\n",
    "else:\n",
    "    print(\"No saved model found. Using current model state.\")\n",
    "\n",
    "# Visualize predictions on test data\n",
    "if 'test_loader' in locals():\n",
    "    print(\"\\nModel Predictions Visualization\")\n",
    "    print(\"Comparing ground truth vs predictions on test samples...\")\n",
    "    \n",
    "    # Visualize predictions\n",
    "    trainer.visualize_predictions(test_loader, num_samples=4, figsize=(20, 8))\n",
    "    \n",
    "else:\n",
    "    print(\"Test data not available for prediction visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70584fa",
   "metadata": {},
   "source": [
    "### Detailed Prediction Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1cadc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed evaluation on test set\n",
    "if 'test_loader' in locals():\n",
    "    print(\"Detailed Model Evaluation\")\n",
    "    \n",
    "    # Evaluate model on test set\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    \n",
    "    # Initialize confusion matrix for class-wise metrics\n",
    "    num_classes = NUM_CLASSES\n",
    "    confusion_matrix = torch.zeros(num_classes, num_classes, device=device)\n",
    "    criterion = nn.CrossEntropyLoss(reduction='sum')\n",
    "    total_pixels = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, targets in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "            images = images.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            with torch.no_grad():  \n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, targets)\n",
    "            \n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            predictions = torch.argmax(outputs, dim=1)\n",
    "            \n",
    "            pred_flat = predictions.reshape(-1)\n",
    "            target_flat = targets.reshape(-1)\n",
    "            \n",
    "            # Update histogram\n",
    "            for t in range(num_classes):\n",
    "                mask = (target_flat == t)\n",
    "                if mask.sum() > 0:\n",
    "                    p = pred_flat[mask]\n",
    "                    bincount = torch.bincount(p, minlength=num_classes)\n",
    "                    confusion_matrix[t] += bincount\n",
    "            \n",
    "            total_pixels += targets.numel()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    test_loss /= total_pixels\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    overall_acc = torch.diag(confusion_matrix).sum() / confusion_matrix.sum()\n",
    "    \n",
    "    print(f\"\\nTest Set Results:\")\n",
    "    print(f\"  Overall Accuracy: {overall_acc.item():.3f}\")\n",
    "    print(f\"  Average Loss: {test_loss:.4f}\")\n",
    "    \n",
    "    # Calculate class-wise IoU\n",
    "    class_ious = []\n",
    "    print(f\"\\nClass-wise IoU:\")\n",
    "    \n",
    "    for class_id in range(num_classes):\n",
    "        # True positives: diagonal elements of the confusion matrix\n",
    "        tp = confusion_matrix[class_id, class_id].item()\n",
    "        \n",
    "        # Sum over row and column for class i\n",
    "        # Row sum = all actual instances of class i\n",
    "        # Column sum = all predicted instances of class iF\n",
    "        row_sum = confusion_matrix[class_id, :].sum().item()\n",
    "        col_sum = confusion_matrix[:, class_id].sum().item()\n",
    "        \n",
    "        # IoU = TP / (TP + FP + FN) = TP / (row_sum + col_sum - TP)\n",
    "        denominator = row_sum + col_sum - tp\n",
    "        iou = tp / denominator if denominator > 0 else 0.0\n",
    "        \n",
    "        class_ious.append(iou)\n",
    "        print(f\"  {class_id}: {CLASS_NAMES[class_id]:<20} IoU: {iou:.3f}\")\n",
    "    \n",
    "    mean_iou = sum(class_ious) / len(class_ious)\n",
    "    print(f\"\\nMean IoU: {mean_iou:.3f}\")\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    confusion_norm = confusion_matrix / confusion_matrix.sum(dim=1, keepdim=True)\n",
    "    confusion_np = confusion_norm.cpu().numpy()\n",
    "    \n",
    "    sns.heatmap(confusion_np, annot=True, fmt='.2f', cmap='Blues',\n",
    "                xticklabels=CLASS_NAMES, yticklabels=CLASS_NAMES)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Ground Truth')\n",
    "    plt.title('Normalized Confusion Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"Test data not available for detailed evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c5a840",
   "metadata": {},
   "source": [
    "# Test Time Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff43388",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tta_utils import TTAPredictor, TTAEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e469977",
   "metadata": {},
   "source": [
    "## Model Saving and Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e4039a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "import os\n",
    "\n",
    "# Create models directory if it doesn't exist\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "# Save model state dict\n",
    "model_path = 'models/ddcm_net_trained85v2tta.pth'\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'training_history': history,\n",
    "    'class_names': CLASS_NAMES\n",
    "}, model_path)\n",
    "\n",
    "print(f\"Model saved to: {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c63cab",
   "metadata": {},
   "source": [
    "### Load a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8efcef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_trained_model(model_path):\n",
    "#     \"\"\"Load a trained DDCM-Net model\"\"\"\n",
    "#     checkpoint = torch.load(model_path, map_location=device, weights_only=False)\n",
    "    \n",
    "#     # Recreate model with same config\n",
    "#     config = checkpoint['model_config']\n",
    "#     loaded_model = DDCMNet(\n",
    "#         num_classes=config['num_classes'],\n",
    "#         variant=config['variant']\n",
    "#     )\n",
    "    \n",
    "#     # Load weights\n",
    "#     loaded_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "#     loaded_model.to(device)\n",
    "#     loaded_model.eval()\n",
    "    \n",
    "#     print(f\"Model loaded from: {model_path}\")\n",
    "#     print(f\"Configuration: {config}\")\n",
    "    \n",
    "#     return loaded_model, checkpoint['training_history']\n",
    "\n",
    "# model_path = '/models/ddcm_net_trained.pth'\n",
    "# model, history = load_trained_model(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179923dd",
   "metadata": {},
   "source": [
    "# Enhanced DDCM-Net with Global Context via Self-Attention\n",
    "\n",
    "This section demonstrates the enhanced DDCM-Net model that integrates transformer-style self-attention blocks to provide global context modeling. The enhancement improves long-range dependency modeling while maintaining computational efficiency through windowed attention mechanisms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff090565",
   "metadata": {},
   "source": [
    "## Model Variants Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c66cfe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Enhanced DDCM-Net Model Comparison ===\\n\")\n",
    "\n",
    "# Create different model variants for comparison\n",
    "print(\"1. Creating model variants...\")\n",
    "\n",
    "# Original DDCM-Net\n",
    "model_base = create_model(variant='base', num_classes=NUM_CLASSES, backbone=BACKBONE)\n",
    "base_params = sum(p.numel() for p in model_base.parameters())\n",
    "print(f\"   Base DDCM-Net: {base_params:,} parameters\")\n",
    "\n",
    "# Enhanced DDCM-Net with default global context\n",
    "model_enhanced = create_model(variant='enhanced', num_classes=NUM_CLASSES, backbone=BACKBONE)\n",
    "enhanced_params = sum(p.numel() for p in model_enhanced.parameters())\n",
    "print(f\"   Enhanced DDCM-Net: {enhanced_params:,} parameters (+{enhanced_params - base_params:,})\")\n",
    "\n",
    "# Enhanced DDCM-Net with custom configuration\n",
    "custom_config = {\n",
    "    'num_heads': 8,           # Number of attention heads\n",
    "    'num_layers': 2,          # Number of transformer layers  \n",
    "    'use_windowed': True,     # Use windowed attention for efficiency\n",
    "    'window_size': 7,         # Window size for windowed attention\n",
    "    'dropout': 0.1,           # Dropout rate\n",
    "    'pos_embed': True         # Use positional embeddings\n",
    "}\n",
    "\n",
    "model_custom = create_model(\n",
    "    variant='enhanced', \n",
    "    num_classes=NUM_CLASSES, \n",
    "    backbone=BACKBONE,\n",
    "    global_context_config=custom_config\n",
    ")\n",
    "custom_params = sum(p.numel() for p in model_custom.parameters())\n",
    "print(f\"   Custom Enhanced DDCM-Net: {custom_params:,} parameters\")\n",
    "\n",
    "# Test forward pass compatibility\n",
    "print(f\"\\n2. Testing forward pass compatibility...\")\n",
    "sample_input = torch.randn(2, 3, 512, 512)\n",
    "print(f\"   Input shape: {sample_input.shape}\")\n",
    "\n",
    "model_base.eval()\n",
    "model_enhanced.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    output_base = model_base(sample_input)\n",
    "    output_enhanced = model_enhanced(sample_input)\n",
    "\n",
    "print(f\"   Base model output: {output_base.shape}\")\n",
    "print(f\"   Enhanced model output: {output_enhanced.shape}\")\n",
    "print(f\"   Output shapes match: {output_base.shape == output_enhanced.shape}\")\n",
    "\n",
    "# Show parameter breakdown\n",
    "print(f\"\\n3. Parameter Analysis...\")\n",
    "parameter_overhead = ((enhanced_params - base_params) / base_params * 100)\n",
    "print(f\"   Parameter overhead: {parameter_overhead:.1f}%\")\n",
    "print(f\"   Additional parameters: {enhanced_params - base_params:,}\")\n",
    "\n",
    "# Show different configuration options\n",
    "print(f\"\\n4. Available Configuration Examples...\")\n",
    "configs = [\n",
    "    {\n",
    "        'name': 'Lightweight (for limited compute)',\n",
    "        'config': {'num_heads': 4, 'num_layers': 1, 'window_size': 7}\n",
    "    },\n",
    "    {\n",
    "        'name': 'Standard (recommended)',\n",
    "        'config': {'num_heads': 8, 'num_layers': 2, 'window_size': 7}\n",
    "    },\n",
    "    {\n",
    "        'name': 'High-capacity (for best accuracy)',\n",
    "        'config': {'num_heads': 8, 'num_layers': 3, 'window_size': 14}\n",
    "    }\n",
    "]\n",
    "\n",
    "for config_info in configs:\n",
    "    name = config_info['name']\n",
    "    config = config_info['config']\n",
    "    \n",
    "    test_model = create_model(\n",
    "        variant='enhanced', \n",
    "        num_classes=NUM_CLASSES,\n",
    "        global_context_config=config\n",
    "    )\n",
    "    params = sum(p.numel() for p in test_model.parameters())\n",
    "    overhead = ((params - base_params) / base_params * 100)\n",
    "    \n",
    "    print(f\"   {name}: {params:,} params (+{overhead:.1f}%)\")\n",
    "\n",
    "print(f\"\\n=== Model variants created successfully! ===\")\n",
    "print(\"The enhanced model is backward compatible and uses the same training interface.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd9d3d4",
   "metadata": {},
   "source": [
    "## Enhanced Model Training\n",
    "\n",
    "Now let's train the enhanced model with the same configuration as the base model to compare performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c732ec0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced Model Training Configuration\n",
    "TRAIN_ENHANCED = True  # Set to True to train the enhanced model\n",
    "ENHANCED_EPOCHS = 80\n",
    "ENHANCED_LR = 8.5e-5 / np.sqrt(2) \n",
    "ENHANCED_WEIGHT_DECAY = 2e-5\n",
    "USE_ENHANCED_DUAL_LR = True  \n",
    "\n",
    "if TRAIN_ENHANCED:\n",
    "    if 'train_loader' in locals() and 'val_loader' in locals():\n",
    "        print(\"=== Training Enhanced DDCM-Net ===\")\n",
    "        \n",
    "        # Create enhanced model with standard configuration\n",
    "        enhanced_model = create_model(\n",
    "            variant='enhanced',\n",
    "            num_classes=NUM_CLASSES,\n",
    "            backbone=BACKBONE,\n",
    "            pretrained=PRETRAINED,\n",
    "            global_context_config={\n",
    "                'num_heads': 8,\n",
    "                'num_layers': 2,\n",
    "                'use_windowed': True,\n",
    "                'window_size': 7,\n",
    "                'dropout': 0.1,\n",
    "                'pos_embed': True\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # Create enhanced trainer\n",
    "        enhanced_trainer = create_trainer(\n",
    "            model=enhanced_model,\n",
    "            device=device,\n",
    "            class_names=CLASS_NAMES\n",
    "        )\n",
    "        \n",
    "        # Print model comparison\n",
    "        enhanced_params = sum(p.numel() for p in enhanced_model.parameters())\n",
    "        base_params = sum(p.numel() for p in model.parameters())\n",
    "        print(f\"Enhanced model parameters: {enhanced_params:,}\")\n",
    "        print(f\"Base model parameters: {base_params:,}\")\n",
    "        print(f\"Parameter increase: +{enhanced_params - base_params:,} (+{((enhanced_params - base_params) / base_params * 100):.1f}%)\")\n",
    "        \n",
    "        print(f\"\\nStarting enhanced model training...\")\n",
    "        print(f\"Configuration:\")\n",
    "        print(f\"  Epochs: {ENHANCED_EPOCHS}\")\n",
    "        print(f\"  Learning rate: {ENHANCED_LR:.2e}\")\n",
    "        print(f\"  Weight decay: {ENHANCED_WEIGHT_DECAY}\")\n",
    "        print(f\"  Device: {device}\")\n",
    "        \n",
    "        # Train the enhanced model\n",
    "        enhanced_history = enhanced_trainer.fit(\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            epochs=ENHANCED_EPOCHS,\n",
    "            lr=ENHANCED_LR,\n",
    "            weight_decay=ENHANCED_WEIGHT_DECAY,\n",
    "            use_mfb=True,\n",
    "            lr_scheduler='step',  # Will be overridden by use_dual_lr\n",
    "            use_dual_lr=USE_ENHANCED_DUAL_LR \n",
    "        )\n",
    "        \n",
    "        # Save enhanced model with different name\n",
    "        enhanced_trainer.save_model('best_enhanced_model.pth')\n",
    "        print(\"Enhanced model saved as 'best_enhanced_model.pth'\")\n",
    "        \n",
    "    else:\n",
    "        print(\"Cannot start enhanced training - dataset not available\")\n",
    "else:\n",
    "    print(\"Enhanced model training is disabled.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79699ef4",
   "metadata": {},
   "source": [
    "### Enhanced Model Training Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54e2518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot enhanced model training history and compare with base model\n",
    "if 'enhanced_history' in locals() and enhanced_trainer.history['train_loss']:\n",
    "    print(\"Enhanced Model Training History Visualization\")\n",
    "    \n",
    "    # Plot enhanced training history\n",
    "    enhanced_trainer.plot_training_history(figsize=(18, 6))\n",
    "    \n",
    "    # Print final enhanced metrics\n",
    "    final_enhanced_train_loss = enhanced_trainer.history['train_loss'][-1]\n",
    "    final_enhanced_val_loss = enhanced_trainer.history['val_loss'][-1]\n",
    "    final_enhanced_train_acc = enhanced_trainer.history['train_acc'][-1]\n",
    "    final_enhanced_val_acc = enhanced_trainer.history['val_acc'][-1]\n",
    "    final_enhanced_train_miou = enhanced_trainer.history['train_miou'][-1]\n",
    "    final_enhanced_val_miou = enhanced_trainer.history['val_miou'][-1]\n",
    "    \n",
    "    print(\"\\nFinal Enhanced Model Metrics:\")\n",
    "    print(f\"  Train Loss: {final_enhanced_train_loss:.4f} | Val Loss: {final_enhanced_val_loss:.4f}\")\n",
    "    print(f\"  Train Acc:  {final_enhanced_train_acc:.3f} | Val Acc:  {final_enhanced_val_acc:.3f}\")\n",
    "    print(f\"  Train mIoU: {final_enhanced_train_miou:.3f} | Val mIoU: {final_enhanced_val_miou:.3f}\")\n",
    "    \n",
    "    # Find best epoch for enhanced model\n",
    "    best_enhanced_epoch = np.argmax(enhanced_trainer.history['val_miou']) + 1\n",
    "    best_enhanced_miou = max(enhanced_trainer.history['val_miou'])\n",
    "    print(f\"\\nBest Enhanced Validation mIoU: {best_enhanced_miou:.3f} (Epoch {best_enhanced_epoch})\")\n",
    "    \n",
    "    # Compare with base model if available\n",
    "    if 'trainer' in locals() and trainer.history['val_miou']:\n",
    "        base_best_miou = max(trainer.history['val_miou'])\n",
    "        improvement = best_enhanced_miou - base_best_miou\n",
    "        print(f\"\\nModel Comparison:\")\n",
    "        print(f\"  Base Model Best mIoU: {base_best_miou:.3f}\")\n",
    "        print(f\"  Enhanced Model Best mIoU: {best_enhanced_miou:.3f}\")\n",
    "        print(f\"  Improvement: {improvement:+.3f} ({(improvement/base_best_miou*100):+.1f}%)\")\n",
    "        \n",
    "        # Side-by-side training curves comparison\n",
    "        if len(trainer.history['val_miou']) > 0 and len(enhanced_trainer.history['val_miou']) > 0:\n",
    "            fig, axes = plt.subplots(1, 3, figsize=(21, 6))\n",
    "            \n",
    "            # Get epochs for both models\n",
    "            base_epochs = range(1, len(trainer.history['train_loss']) + 1)\n",
    "            enhanced_epochs = range(1, len(enhanced_trainer.history['train_loss']) + 1)\n",
    "            \n",
    "            # Loss comparison\n",
    "            axes[0].plot(base_epochs, trainer.history['train_loss'], 'b-', label='Base Train', linewidth=2)\n",
    "            axes[0].plot(base_epochs, trainer.history['val_loss'], 'b--', label='Base Val', linewidth=2)\n",
    "            axes[0].plot(enhanced_epochs, enhanced_trainer.history['train_loss'], 'r-', label='Enhanced Train', linewidth=2)\n",
    "            axes[0].plot(enhanced_epochs, enhanced_trainer.history['val_loss'], 'r--', label='Enhanced Val', linewidth=2)\n",
    "            axes[0].set_title('Loss Comparison')\n",
    "            axes[0].set_xlabel('Epoch')\n",
    "            axes[0].set_ylabel('Loss')\n",
    "            axes[0].legend()\n",
    "            axes[0].grid(True, alpha=0.3)\n",
    "            \n",
    "            # Accuracy comparison\n",
    "            axes[1].plot(base_epochs, trainer.history['train_acc'], 'b-', label='Base Train', linewidth=2)\n",
    "            axes[1].plot(base_epochs, trainer.history['val_acc'], 'b--', label='Base Val', linewidth=2)\n",
    "            axes[1].plot(enhanced_epochs, enhanced_trainer.history['train_acc'], 'r-', label='Enhanced Train', linewidth=2)\n",
    "            axes[1].plot(enhanced_epochs, enhanced_trainer.history['val_acc'], 'r--', label='Enhanced Val', linewidth=2)\n",
    "            axes[1].set_title('Accuracy Comparison')\n",
    "            axes[1].set_xlabel('Epoch')\n",
    "            axes[1].set_ylabel('Accuracy')\n",
    "            axes[1].legend()\n",
    "            axes[1].grid(True, alpha=0.3)\n",
    "            \n",
    "            # mIoU comparison\n",
    "            axes[2].plot(base_epochs, trainer.history['train_miou'], 'b-', label='Base Train', linewidth=2)\n",
    "            axes[2].plot(base_epochs, trainer.history['val_miou'], 'b--', label='Base Val', linewidth=2)\n",
    "            axes[2].plot(enhanced_epochs, enhanced_trainer.history['train_miou'], 'r-', label='Enhanced Train', linewidth=2)\n",
    "            axes[2].plot(enhanced_epochs, enhanced_trainer.history['val_miou'], 'r--', label='Enhanced Val', linewidth=2)\n",
    "            axes[2].set_title('mIoU Comparison')\n",
    "            axes[2].set_xlabel('Epoch')\n",
    "            axes[2].set_ylabel('mIoU')\n",
    "            axes[2].legend()\n",
    "            axes[2].grid(True, alpha=0.3)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"No enhanced training history available.\")\n",
    "    print(\"Please run enhanced training first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec7b03b",
   "metadata": {},
   "source": [
    "## Test Time Augmentation (TTA) Comparison\n",
    "\n",
    "Now let's compare both the base and enhanced models using Test Time Augmentation to see the performance differences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6316b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import TTA utilities\n",
    "from tta_utils import TTAEvaluator, load_model_for_tta\n",
    "\n",
    "print(\"=== Test Time Augmentation Model Comparison ===\\n\")\n",
    "\n",
    "# TTA Configuration\n",
    "RUN_TTA_COMPARISON = True  # Set to True to enable TTA comparison\n",
    "MAX_TTA_IMAGES = 3         # Limit images for faster demonstration\n",
    "TTA_PATCH_SIZE = 448       \n",
    "TTA_STRIDE = 100           \n",
    "TTA_VISUALIZE = True       # Show visualizations for each image\n",
    "\n",
    "if RUN_TTA_COMPARISON:\n",
    "    if 'test_loader' in locals():\n",
    "        print(\"Preparing models for TTA comparison...\")\n",
    "        \n",
    "        # Create TTA evaluator\n",
    "        tta_evaluator = TTAEvaluator(class_names=CLASS_NAMES)\n",
    "        \n",
    "        # Prepare models dictionary\n",
    "        models_to_compare = {}\n",
    "        \n",
    "        # Add base model (current trained model)\n",
    "        if 'trainer' in locals() and os.path.exists('best_model.pth'):\n",
    "            print(\"Loading base model...\")\n",
    "            base_model_for_tta = load_model_for_tta(\n",
    "                'best_model.pth', \n",
    "                variant='base', \n",
    "                num_classes=NUM_CLASSES, \n",
    "                backbone=BACKBONE,\n",
    "                device=device\n",
    "            )\n",
    "            models_to_compare['Base DDCM-Net'] = base_model_for_tta\n",
    "        \n",
    "        # Add enhanced model if available\n",
    "        if 'enhanced_trainer' in locals() and os.path.exists('best_enhanced_model.pth'):\n",
    "            print(\"Loading enhanced model...\")\n",
    "            enhanced_model_for_tta = load_model_for_tta(\n",
    "                'best_enhanced_model.pth',\n",
    "                variant='enhanced',\n",
    "                num_classes=NUM_CLASSES,\n",
    "                backbone=BACKBONE,\n",
    "                device=device\n",
    "            )\n",
    "            models_to_compare['Enhanced DDCM-Net'] = enhanced_model_for_tta\n",
    "        elif 'enhanced_model' in locals():\n",
    "            print(\"Using current enhanced model...\")\n",
    "            models_to_compare['Enhanced DDCM-Net'] = enhanced_model\n",
    "        \n",
    "        if len(models_to_compare) > 0:\n",
    "            print(f\"Models to compare: {list(models_to_compare.keys())}\")\n",
    "            print(f\"TTA Configuration:\")\n",
    "            print(f\"  Patch size: {TTA_PATCH_SIZE}×{TTA_PATCH_SIZE}\")\n",
    "            print(f\"  Stride: {TTA_STRIDE} pixels\")\n",
    "            print(f\"  Max images: {MAX_TTA_IMAGES}\")\n",
    "            print(f\"  Visualizations: {'Enabled' if TTA_VISUALIZE else 'Disabled'}\")\n",
    "            \n",
    "            # Run TTA comparison\n",
    "            comparison_results = tta_evaluator.compare_models_with_tta(\n",
    "                models_dict=models_to_compare,\n",
    "                test_loader=test_loader,\n",
    "                max_images=MAX_TTA_IMAGES,\n",
    "                visualize=TTA_VISUALIZE,\n",
    "                patch_size=TTA_PATCH_SIZE,\n",
    "                stride=TTA_STRIDE\n",
    "            )\n",
    "            \n",
    "            print(\"\\n=== TTA Comparison Complete ===\")\n",
    "            \n",
    "        else:\n",
    "            print(\"No trained models available for TTA comparison.\")\n",
    "            print(\"Please train at least one model first.\")\n",
    "            \n",
    "    else:\n",
    "        print(\"Test loader not available.\")\n",
    "        print(\"Please run the dataset loading cells first to enable TTA evaluation.\")\n",
    "else:\n",
    "    print(\"TTA comparison is disabled.\")\n",
    "    print(\"Set RUN_TTA_COMPARISON = True to enable TTA model comparison.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac4c6b3",
   "metadata": {},
   "source": [
    "### TTA Results Visualization and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900181e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze and visualize TTA comparison results\n",
    "if 'comparison_results' in locals() and comparison_results:\n",
    "    print(\"=== TTA Results Analysis ===\\n\")\n",
    "    \n",
    "    # Extract model names and results\n",
    "    model_names = list(comparison_results.keys())\n",
    "    \n",
    "    if len(model_names) >= 2:\n",
    "        # Create comprehensive comparison visualization\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "        \n",
    "        # Prepare data for plotting\n",
    "        models_data = []\n",
    "        for model_name in model_names:\n",
    "            results = comparison_results[model_name]\n",
    "            models_data.append({\n",
    "                'name': model_name,\n",
    "                'reg_accs': results['all_reg_accuracies'],\n",
    "                'tta_accs': results['all_tta_accuracies'],\n",
    "                'reg_ious': results['all_reg_ious'],\n",
    "                'tta_ious': results['all_tta_ious'],\n",
    "                'avg_reg_acc': results['avg_reg_acc'],\n",
    "                'avg_tta_acc': results['avg_tta_acc'],\n",
    "                'avg_reg_miou': results['avg_reg_miou'],\n",
    "                'avg_tta_miou': results['avg_tta_miou']\n",
    "            })\n",
    "        \n",
    "        # Plot 1: Accuracy comparison per image\n",
    "        for i, model_data in enumerate(models_data):\n",
    "            x_positions = range(len(model_data['reg_accs']))\n",
    "            axes[0, 0].scatter([x + i*0.1 for x in x_positions], model_data['reg_accs'], \n",
    "                              alpha=0.7, label=f\"{model_data['name']} Regular\", marker='o')\n",
    "            axes[0, 0].scatter([x + i*0.1 for x in x_positions], model_data['tta_accs'], \n",
    "                              alpha=0.7, label=f\"{model_data['name']} TTA\", marker='^')\n",
    "        \n",
    "        axes[0, 0].set_xlabel('Image Index')\n",
    "        axes[0, 0].set_ylabel('Accuracy')\n",
    "        axes[0, 0].set_title('Accuracy Comparison Across Images')\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 2: mIoU comparison per image\n",
    "        for i, model_data in enumerate(models_data):\n",
    "            x_positions = range(len(model_data['reg_ious']))\n",
    "            axes[0, 1].scatter([x + i*0.1 for x in x_positions], model_data['reg_ious'], \n",
    "                              alpha=0.7, label=f\"{model_data['name']} Regular\", marker='o')\n",
    "            axes[0, 1].scatter([x + i*0.1 for x in x_positions], model_data['tta_ious'], \n",
    "                              alpha=0.7, label=f\"{model_data['name']} TTA\", marker='^')\n",
    "        \n",
    "        axes[0, 1].set_xlabel('Image Index')\n",
    "        axes[0, 1].set_ylabel('mIoU')\n",
    "        axes[0, 1].set_title('mIoU Comparison Across Images')\n",
    "        axes[0, 1].legend()\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 3: Average performance comparison (bar chart)\n",
    "        x_pos = np.arange(len(model_names))\n",
    "        width = 0.35\n",
    "        \n",
    "        reg_accs = [model_data['avg_reg_acc'] for model_data in models_data]\n",
    "        tta_accs = [model_data['avg_tta_acc'] for model_data in models_data]\n",
    "        \n",
    "        bars1 = axes[1, 0].bar([x - width/2 for x in x_pos], reg_accs, width, label='Regular', alpha=0.8)\n",
    "        bars2 = axes[1, 0].bar([x + width/2 for x in x_pos], tta_accs, width, label='TTA', alpha=0.8)\n",
    "        \n",
    "        axes[1, 0].set_xlabel('Model')\n",
    "        axes[1, 0].set_ylabel('Average Accuracy')\n",
    "        axes[1, 0].set_title('Average Accuracy Comparison')\n",
    "        axes[1, 0].set_xticks(x_pos)\n",
    "        axes[1, 0].set_xticklabels(model_names)\n",
    "        axes[1, 0].legend()\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar in bars1 + bars2:\n",
    "            height = bar.get_height()\n",
    "            axes[1, 0].annotate(f'{height:.3f}',\n",
    "                               xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                               xytext=(0, 3),  # 3 points vertical offset\n",
    "                               textcoords=\"offset points\",\n",
    "                               ha='center', va='bottom', fontsize=9)\n",
    "        \n",
    "        # Plot 4: Average mIoU comparison (bar chart)\n",
    "        reg_ious = [model_data['avg_reg_miou'] for model_data in models_data]\n",
    "        tta_ious = [model_data['avg_tta_miou'] for model_data in models_data]\n",
    "        \n",
    "        bars3 = axes[1, 1].bar([x - width/2 for x in x_pos], reg_ious, width, label='Regular', alpha=0.8)\n",
    "        bars4 = axes[1, 1].bar([x + width/2 for x in x_pos], tta_ious, width, label='TTA', alpha=0.8)\n",
    "        \n",
    "        axes[1, 1].set_xlabel('Model')\n",
    "        axes[1, 1].set_ylabel('Average mIoU')\n",
    "        axes[1, 1].set_title('Average mIoU Comparison')\n",
    "        axes[1, 1].set_xticks(x_pos)\n",
    "        axes[1, 1].set_xticklabels(model_names)\n",
    "        axes[1, 1].legend()\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar in bars3 + bars4:\n",
    "            height = bar.get_height()\n",
    "            axes[1, 1].annotate(f'{height:.3f}',\n",
    "                               xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                               xytext=(0, 3),  # 3 points vertical offset\n",
    "                               textcoords=\"offset points\",\n",
    "                               ha='center', va='bottom', fontsize=9)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print detailed comparison\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"DETAILED COMPARISON ANALYSIS\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        for i, model_data in enumerate(models_data):\n",
    "            print(f\"\\n{model_data['name']}:\")\n",
    "            print(f\"  Regular -> TTA Accuracy: {model_data['avg_reg_acc']:.4f} -> {model_data['avg_tta_acc']:.4f} \"\n",
    "                  f\"(+{model_data['avg_tta_acc'] - model_data['avg_reg_acc']:.4f})\")\n",
    "            print(f\"  Regular -> TTA mIoU: {model_data['avg_reg_miou']:.4f} -> {model_data['avg_tta_miou']:.4f} \"\n",
    "                  f\"(+{model_data['avg_tta_miou'] - model_data['avg_reg_miou']:.4f})\")\n",
    "        \n",
    "        # Compare models against each other\n",
    "        if len(models_data) == 2:\n",
    "            base_model = models_data[0]\n",
    "            enhanced_model = models_data[1]\n",
    "            \n",
    "            print(f\"\\n\" + \"=\"*40)\n",
    "            print(f\"MODEL vs MODEL COMPARISON\")\n",
    "            print(f\"=\"*40)\n",
    "            \n",
    "            # Regular prediction comparison\n",
    "            reg_acc_diff = enhanced_model['avg_reg_acc'] - base_model['avg_reg_acc']\n",
    "            reg_miou_diff = enhanced_model['avg_reg_miou'] - base_model['avg_reg_miou']\n",
    "            \n",
    "            print(f\"Regular Prediction Comparison:\")\n",
    "            print(f\"  Accuracy: {enhanced_model['name']} vs {base_model['name']} = {reg_acc_diff:+.4f}\")\n",
    "            print(f\"  mIoU: {enhanced_model['name']} vs {base_model['name']} = {reg_miou_diff:+.4f}\")\n",
    "            \n",
    "            # TTA prediction comparison\n",
    "            tta_acc_diff = enhanced_model['avg_tta_acc'] - base_model['avg_tta_acc']\n",
    "            tta_miou_diff = enhanced_model['avg_tta_miou'] - base_model['avg_tta_miou']\n",
    "            \n",
    "            print(f\"\\nTTA Prediction Comparison:\")\n",
    "            print(f\"  Accuracy: {enhanced_model['name']} vs {base_model['name']} = {tta_acc_diff:+.4f}\")\n",
    "            print(f\"  mIoU: {enhanced_model['name']} vs {base_model['name']} = {tta_miou_diff:+.4f}\")\n",
    "            \n",
    "            # Overall assessment\n",
    "            print(f\"\\nOverall Assessment:\")\n",
    "            if tta_acc_diff > 0 and tta_miou_diff > 0:\n",
    "                print(f\"✓ Enhanced model shows improvement in both accuracy and mIoU with TTA\")\n",
    "            elif tta_acc_diff > 0 or tta_miou_diff > 0:\n",
    "                print(f\"~ Enhanced model shows mixed results compared to base model\")\n",
    "            else:\n",
    "                print(f\"- Enhanced model does not show clear improvement with TTA\")\n",
    "    \n",
    "    else:\n",
    "        print(f\"Only {len(model_names)} model(s) available for comparison.\")\n",
    "        print(\"Need at least 2 models for comprehensive comparison.\")\n",
    "\n",
    "else:\n",
    "    print(\"No TTA comparison results available.\")\n",
    "    print(\"Please run the TTA comparison first.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
