{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "973e6747",
   "metadata": {},
   "source": [
    "# DDCM-Net: Dense Dilated Convolutions Merging Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebb804a",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43a8ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from model import create_model, create_trainer\n",
    "from dataset_loader import create_dataloaders\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Check device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f03339",
   "metadata": {},
   "source": [
    "## 2. Model Architecture Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ed5e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "NUM_CLASSES = 6\n",
    "BACKBONE = 'resnet50'\n",
    "PRETRAINED = True\n",
    "\n",
    "# Class names for visualization\n",
    "CLASS_NAMES = [\n",
    "    'Impervious surfaces',  # 0 - White\n",
    "    'Building',             # 1 - Blue  \n",
    "    'Low vegetation',       # 2 - Cyan\n",
    "    'Tree',                 # 3 - Green\n",
    "    'Car',                  # 4 - Yellow\n",
    "    'Clutter/background'    # 5 - Red\n",
    "]\n",
    "\n",
    "# Create model\n",
    "model = create_model(\n",
    "    variant='base',\n",
    "    num_classes=NUM_CLASSES,\n",
    "    backbone=BACKBONE,\n",
    "    pretrained=PRETRAINED\n",
    ")\n",
    "\n",
    "# Create trainer wrapper\n",
    "trainer = create_trainer(\n",
    "    model=model,\n",
    "    device=device,\n",
    "    class_names=CLASS_NAMES\n",
    ")\n",
    "\n",
    "# Print model information\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Architecture: DDCM-Net with {BACKBONE} backbone\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bc6efb",
   "metadata": {},
   "source": [
    "### Load model from a previous training session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4786af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer = trainer.load_model(\"best_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc7bd78",
   "metadata": {},
   "source": [
    "## 3. Dataset Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8dd4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset configuration\n",
    "DATA_ROOT = \"./data\"\n",
    "DATASET = \"potsdam\"  # or \"vaihingen\" or \"both\"\n",
    "BATCH_SIZE = 5  \n",
    "NUM_WORKERS = 4\n",
    "PATCH_SIZE = 256  \n",
    "\n",
    "print(\"Loading dataset...\")\n",
    "\n",
    "# Check if data exists\n",
    "if not os.path.exists(DATA_ROOT):\n",
    "    print(f\"Data directory not found: {DATA_ROOT}\")\n",
    "    print(\"Please ensure you have the ISPRS dataset in the data folder\")\n",
    "    print(\"Expected structure:\")\n",
    "    print(\"data/\")\n",
    "    print(\"├── potsdam/\")\n",
    "    print(\"│   ├── images/\")\n",
    "    print(\"│   └── labels/\")\n",
    "    print(\"└── vaihingen/\")\n",
    "    print(\"    ├── images/\")\n",
    "    print(\"    └── labels/\")\n",
    "else:\n",
    "    train_loader, val_loader, test_loader, holdout_loader = create_dataloaders(\n",
    "        root_dir=DATA_ROOT,\n",
    "        dataset=DATASET,\n",
    "        patch_size=PATCH_SIZE,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        num_workers=NUM_WORKERS\n",
    "    )\n",
    "    \n",
    "    print(f\"Dataset: {DATASET.upper()}\")\n",
    "    print(f\"Batch size: {BATCH_SIZE}\")\n",
    "    print(f\"Patch size: {PATCH_SIZE}x{PATCH_SIZE}\")\n",
    "    print(f\"Train batches: {len(train_loader):,}\")\n",
    "    print(f\"Validation batches: {len(val_loader):,}\")\n",
    "    print(f\"Test batches: {len(test_loader):,}\")\n",
    "    \n",
    "    # Show a sample batch\n",
    "    # sample_images, sample_labels = next(iter(train_loader))\n",
    "    # print(f\"Sample batch shape - Images: {sample_images.shape}, Labels: {sample_labels.shape}\")\n",
    "    # print(f\"Classes in sample: {torch.unique(sample_labels).tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ef9dea",
   "metadata": {},
   "source": [
    "### Dataset Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ef7ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize dataset samples\n",
    "def visualize_dataset_samples(dataloader, num_samples=4):\n",
    "    \"\"\"Visualize samples from the dataset\"\"\"\n",
    "    fig, axes = plt.subplots(2, num_samples, figsize=(20, 10))\n",
    "    \n",
    "    # Get a batch\n",
    "    images, labels = next(iter(dataloader))\n",
    "    \n",
    "    for i in range(min(num_samples, len(images))):\n",
    "        # Get single sample\n",
    "        img = images[i]\n",
    "        label = labels[i]\n",
    "        \n",
    "        # Denormalize image for visualization\n",
    "        mean = torch.tensor([0.485, 0.456, 0.406])\n",
    "        std = torch.tensor([0.229, 0.224, 0.225])\n",
    "        img_denorm = img * std[:, None, None] + mean[:, None, None]\n",
    "        img_denorm = torch.clamp(img_denorm, 0, 1)\n",
    "        \n",
    "        # Plot image\n",
    "        axes[0, i].imshow(img_denorm.permute(1, 2, 0))\n",
    "        axes[0, i].set_title(f'Image {i+1}')\n",
    "        axes[0, i].axis('off')\n",
    "        \n",
    "        # Plot label\n",
    "        im = axes[1, i].imshow(label, cmap='tab10', vmin=0, vmax=5)\n",
    "        axes[1, i].set_title(f'Ground Truth {i+1}')\n",
    "        axes[1, i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print class statistics\n",
    "    unique_classes = torch.unique(labels)\n",
    "    print(f\"Classes in this batch: {unique_classes.tolist()}\")\n",
    "    for class_id in unique_classes:\n",
    "        class_name = CLASS_NAMES[class_id]\n",
    "        pixel_count = (labels == class_id).sum().item()\n",
    "        percentage = pixel_count / labels.numel() * 100\n",
    "        print(f\"  {class_id}: {class_name} - {pixel_count:,} pixels ({percentage:.1f}%)\")\n",
    "\n",
    "# Visualize samples if data is available\n",
    "if 'train_loader' in locals():\n",
    "    print(\"Dataset samples:\")\n",
    "    visualize_dataset_samples(train_loader, num_samples=4)\n",
    "else:\n",
    "    print(\"Dataset not loaded. Please ensure data is available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca978b5",
   "metadata": {},
   "source": [
    "## 4. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7a37ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "EPOCHS = 80\n",
    "LEARNING_RATE = 8.5e-5  \n",
    "WEIGHT_DECAY = 2e-5\n",
    "\n",
    "print(\"Starting model training...\")\n",
    "print(f\"Configuration:\")\n",
    "print(f\"Epochs: {EPOCHS}\")\n",
    "print(f\"Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"Weight decay: {WEIGHT_DECAY}\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# Start training if data is available\n",
    "if 'train_loader' in locals() and 'val_loader' in locals():\n",
    "    print(f\"\\nTraining started...\")\n",
    "    \n",
    "    # Train the model\n",
    "    history = trainer.fit(\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        epochs=EPOCHS,\n",
    "        lr=LEARNING_RATE,\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "        use_mfb=True,\n",
    "        lr_scheduler='poly'\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nTraining completed!\")\n",
    "    \n",
    "else:\n",
    "    print(\"Cannot start training - dataset not available\")\n",
    "    print(\"Please ensure the data is properly loaded before training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5e3778",
   "metadata": {},
   "source": [
    "## 5. Training Visualization and Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576a6d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "if 'history' in locals() and trainer.history['train_loss']:\n",
    "    print(\"Training History Visualization\")\n",
    "    \n",
    "    # Plot using trainer's built-in visualization\n",
    "    trainer.plot_training_history(figsize=(18, 6))\n",
    "    \n",
    "    # Print final metrics\n",
    "    final_train_loss = trainer.history['train_loss'][-1]\n",
    "    final_val_loss = trainer.history['val_loss'][-1]\n",
    "    final_train_acc = trainer.history['train_acc'][-1]\n",
    "    final_val_acc = trainer.history['val_acc'][-1]\n",
    "    final_train_miou = trainer.history['train_miou'][-1]\n",
    "    final_val_miou = trainer.history['val_miou'][-1]\n",
    "    \n",
    "    print(\"\\nFinal Training Metrics:\")\n",
    "    print(f\"  Train Loss: {final_train_loss:.4f} | Val Loss: {final_val_loss:.4f}\")\n",
    "    print(f\"  Train Acc:  {final_train_acc:.3f} | Val Acc:  {final_val_acc:.3f}\")\n",
    "    print(f\"  Train mIoU: {final_train_miou:.3f} | Val mIoU: {final_val_miou:.3f}\")\n",
    "    \n",
    "    # Find best epoch\n",
    "    best_epoch = np.argmax(trainer.history['val_miou']) + 1\n",
    "    best_miou = max(trainer.history['val_miou'])\n",
    "    print(f\"\\nBest Validation mIoU: {best_miou:.3f} (Epoch {best_epoch})\")\n",
    "    \n",
    "else:\n",
    "    print(\"No training history available.\")\n",
    "    print(\"Please run training first or load a pre-trained model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617474b9",
   "metadata": {},
   "source": [
    "## 5.5. Continue Training from Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64816604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for continued training\n",
    "CONTINUE_TRAINING = True  # Set to True to enable\n",
    "ADDITIONAL_EPOCHS = 30     \n",
    "LAST_COMPLETED_EPOCH = 60 \n",
    "LEARNING_RATE = 8.5e-5 / np.sqrt(2) \n",
    "WEIGHT_DECAY = 2e-5\n",
    "\n",
    "if CONTINUE_TRAINING:\n",
    "    if 'trainer' in locals() and 'train_loader' in locals() and 'val_loader' in locals():\n",
    "        # Load the saved model\n",
    "        if os.path.exists('best_model.pth'):\n",
    "            print(\"Loading best model for continued training...\")\n",
    "            trainer.load_model('best_model.pth')\n",
    "            \n",
    "            # Show current training state\n",
    "            total_epochs_trained = len(trainer.history['train_loss'])\n",
    "            print(f\"Model has been trained for {total_epochs_trained} epochs\")\n",
    "            \n",
    "            if trainer.history['val_miou']:\n",
    "                current_best = max(trainer.history['val_miou'])\n",
    "                print(f\"Current best validation mIoU: {current_best:.3f}\")\n",
    "            \n",
    "            print(f\"\\nContinuing training for {ADDITIONAL_EPOCHS} more epochs...\")\n",
    "            updated_history = trainer.continue_training(\n",
    "                train_loader=train_loader, \n",
    "                val_loader=val_loader,\n",
    "                additional_epochs=ADDITIONAL_EPOCHS,\n",
    "                current_epoch=LAST_COMPLETED_EPOCH,  # Use this to set the LR correctly\n",
    "                initial_lr=LEARNING_RATE,\n",
    "                weight_decay=WEIGHT_DECAY\n",
    "            )\n",
    "            \n",
    "            # Plot updated training history\n",
    "            print(\"\\nUpdated Training History:\")\n",
    "            trainer.plot_training_history(figsize=(18, 6))\n",
    "            \n",
    "        else:\n",
    "            print(\"No saved model found at 'best_model.pth'!\")\n",
    "            print(\"Please train the model first or check the model path.\")\n",
    "    else:\n",
    "        print(\"Dataloaders not found!\")\n",
    "        print(\"Please ensure 'trainer', 'train_loader', and 'val_loader' are available.\")\n",
    "else:\n",
    "    print(\"Continued training is disabled.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420d790a",
   "metadata": {},
   "source": [
    "## 6. Model Prediction and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88182652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model for inference\n",
    "if os.path.exists('best_model.pth'):\n",
    "    print(\"Loading best trained model...\")\n",
    "    trainer.load_model('best_model.pth')\n",
    "    print(\"Model loaded successfully!\")\n",
    "else:\n",
    "    print(\"No saved model found. Using current model state.\")\n",
    "\n",
    "# Visualize predictions on test data\n",
    "if 'test_loader' in locals():\n",
    "    print(\"\\nModel Predictions Visualization\")\n",
    "    print(\"Comparing ground truth vs predictions on test samples...\")\n",
    "    \n",
    "    # Visualize predictions\n",
    "    trainer.visualize_predictions(test_loader, num_samples=4, figsize=(20, 8))\n",
    "    \n",
    "else:\n",
    "    print(\"Test data not available for prediction visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70584fa",
   "metadata": {},
   "source": [
    "### Detailed Prediction Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1cadc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed evaluation on test set\n",
    "if 'test_loader' in locals():\n",
    "    print(\"Detailed Model Evaluation\")\n",
    "    \n",
    "    # Evaluate model on test set\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    \n",
    "    # Initialize confusion matrix for class-wise metrics\n",
    "    num_classes = NUM_CLASSES\n",
    "    confusion_matrix = torch.zeros(num_classes, num_classes, device=device)\n",
    "    criterion = nn.CrossEntropyLoss(reduction='sum')\n",
    "    total_pixels = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, targets in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "            images = images.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            with torch.no_grad():  \n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, targets)\n",
    "            \n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            predictions = torch.argmax(outputs, dim=1)\n",
    "            \n",
    "            pred_flat = predictions.reshape(-1)\n",
    "            target_flat = targets.reshape(-1)\n",
    "            \n",
    "            # Update histogram\n",
    "            for t in range(num_classes):\n",
    "                mask = (target_flat == t)\n",
    "                if mask.sum() > 0:\n",
    "                    p = pred_flat[mask]\n",
    "                    bincount = torch.bincount(p, minlength=num_classes)\n",
    "                    confusion_matrix[t] += bincount\n",
    "            \n",
    "            total_pixels += targets.numel()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    test_loss /= total_pixels\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    overall_acc = torch.diag(confusion_matrix).sum() / confusion_matrix.sum()\n",
    "    \n",
    "    print(f\"\\nTest Set Results:\")\n",
    "    print(f\"  Overall Accuracy: {overall_acc.item():.3f}\")\n",
    "    print(f\"  Average Loss: {test_loss:.4f}\")\n",
    "    \n",
    "    # Calculate class-wise IoU\n",
    "    class_ious = []\n",
    "    print(f\"\\nClass-wise IoU:\")\n",
    "    \n",
    "    for class_id in range(num_classes):\n",
    "        # True positives: diagonal elements of the confusion matrix\n",
    "        tp = confusion_matrix[class_id, class_id].item()\n",
    "        \n",
    "        # Sum over row and column for class i\n",
    "        # Row sum = all actual instances of class i\n",
    "        # Column sum = all predicted instances of class iF\n",
    "        row_sum = confusion_matrix[class_id, :].sum().item()\n",
    "        col_sum = confusion_matrix[:, class_id].sum().item()\n",
    "        \n",
    "        # IoU = TP / (TP + FP + FN) = TP / (row_sum + col_sum - TP)\n",
    "        denominator = row_sum + col_sum - tp\n",
    "        iou = tp / denominator if denominator > 0 else 0.0\n",
    "        \n",
    "        class_ious.append(iou)\n",
    "        print(f\"  {class_id}: {CLASS_NAMES[class_id]:<20} IoU: {iou:.3f}\")\n",
    "    \n",
    "    mean_iou = sum(class_ious) / len(class_ious)\n",
    "    print(f\"\\nMean IoU: {mean_iou:.3f}\")\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    confusion_norm = confusion_matrix / confusion_matrix.sum(dim=1, keepdim=True)\n",
    "    confusion_np = confusion_norm.cpu().numpy()\n",
    "    \n",
    "    sns.heatmap(confusion_np, annot=True, fmt='.2f', cmap='Blues',\n",
    "                xticklabels=CLASS_NAMES, yticklabels=CLASS_NAMES)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Ground Truth')\n",
    "    plt.title('Normalized Confusion Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"Test data not available for detailed evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c5a840",
   "metadata": {},
   "source": [
    "# Test Time Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff43388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paper specification: 448×448 patches with 100-pixel stride, flipping and mirroring\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "\n",
    "def apply_tta_transforms(patch):\n",
    "    \"\"\"\n",
    "    Apply TTA transformations: original, horizontal flip, vertical flip, both flips\n",
    "    \n",
    "    Args:\n",
    "        patch: Input tensor of shape [1, 3, H, W]\n",
    "    \n",
    "    Returns:\n",
    "        List of transformed patches\n",
    "    \"\"\"\n",
    "    transforms = []\n",
    "    \n",
    "    # Original\n",
    "    transforms.append(('original', patch))\n",
    "    \n",
    "    # Horizontal flip (mirroring)\n",
    "    transforms.append(('hflip', torch.flip(patch, [3])))\n",
    "    \n",
    "    # Vertical flip\n",
    "    transforms.append(('vflip', torch.flip(patch, [2])))\n",
    "    \n",
    "    # Both flips\n",
    "    transforms.append(('hvflip', torch.flip(patch, [2, 3])))\n",
    "    \n",
    "    return transforms\n",
    "\n",
    "def reverse_tta_transforms(prediction, transform_name):\n",
    "    \"\"\"\n",
    "    Reverse the TTA transformation on the prediction\n",
    "    \n",
    "    Args:\n",
    "        prediction: Model output tensor [1, num_classes, H, W]\n",
    "        transform_name: Name of the transformation to reverse\n",
    "    \n",
    "    Returns:\n",
    "        Reversed prediction tensor\n",
    "    \"\"\"\n",
    "    if transform_name == 'original':\n",
    "        return prediction\n",
    "    elif transform_name == 'hflip':\n",
    "        return torch.flip(prediction, [3])\n",
    "    elif transform_name == 'vflip':\n",
    "        return torch.flip(prediction, [2])\n",
    "    elif transform_name == 'hvflip':\n",
    "        return torch.flip(prediction, [2, 3])\n",
    "    else:\n",
    "        return prediction\n",
    "\n",
    "def test_time_augmentation_sliding_window(model, image, patch_size=448, stride=100, device='cuda'):\n",
    "    \"\"\"\n",
    "    Apply TTA with sliding windows as described in the paper\n",
    "    \n",
    "    Args:\n",
    "        model: Trained DDCM-Net model\n",
    "        image: Input image tensor [1, 3, H, W] or [3, H, W]\n",
    "        patch_size: Size of sliding window patches (default: 448)\n",
    "        stride: Stride for sliding window (default: 100)\n",
    "        device: Device to run inference on\n",
    "    \n",
    "    Returns:\n",
    "        Final averaged prediction [1, num_classes, H, W]\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Ensure input is 4D [1, 3, H, W]\n",
    "    if len(image.shape) == 3:\n",
    "        image = image.unsqueeze(0)\n",
    "    \n",
    "    batch_size, channels, height, width = image.shape\n",
    "    num_classes = NUM_CLASSES\n",
    "    \n",
    "    # Move to device\n",
    "    image = image.to(device)\n",
    "    \n",
    "    # Initialize output canvas and count map\n",
    "    prediction_canvas = torch.zeros(batch_size, num_classes, height, width, device=device)\n",
    "    count_canvas = torch.zeros(batch_size, 1, height, width, device=device)\n",
    "    \n",
    "    print(f\"Applying TTA with {patch_size}×{patch_size} patches, stride={stride}\")\n",
    "    print(f\"Image size: {height}×{width}\")\n",
    "    \n",
    "    # Calculate positions for sliding windows (ensuring full coverage)\n",
    "    y_positions = []\n",
    "    x_positions = []\n",
    "    \n",
    "    # Generate y positions\n",
    "    for y in range(0, height - patch_size + 1, stride):\n",
    "        y_positions.append(y)\n",
    "    # Ensure we include the bottom edge\n",
    "    if y_positions[-1] + patch_size < height:\n",
    "        y_positions.append(height - patch_size)\n",
    "    \n",
    "    # Generate x positions\n",
    "    for x in range(0, width - patch_size + 1, stride):\n",
    "        x_positions.append(x)\n",
    "    # Ensure we include the right edge\n",
    "    if x_positions[-1] + patch_size < width:\n",
    "        x_positions.append(width - patch_size)\n",
    "    \n",
    "    total_patches = len(y_positions) * len(x_positions)\n",
    "    print(f\"Processing {total_patches} patches ({len(y_positions)}×{len(x_positions)})\")\n",
    "    print(f\"Y positions: {len(y_positions)} (from 0 to {y_positions[-1]})\")\n",
    "    print(f\"X positions: {len(x_positions)} (from 0 to {x_positions[-1]})\")\n",
    "    \n",
    "    patch_count = 0\n",
    "    \n",
    "    # Sliding window extraction with complete coverage\n",
    "    for y in y_positions:\n",
    "        for x in x_positions:\n",
    "            patch_count += 1\n",
    "            \n",
    "            # Extract patch\n",
    "            patch = image[:, :, y:y+patch_size, x:x+patch_size]\n",
    "            \n",
    "            # Apply TTA transformations\n",
    "            transforms = apply_tta_transforms(patch)\n",
    "            \n",
    "            patch_predictions = []\n",
    "            \n",
    "            # Process each transformation\n",
    "            for transform_name, transformed_patch in transforms:\n",
    "                with torch.no_grad():\n",
    "                    # Get model prediction\n",
    "                    pred = model(transformed_patch)\n",
    "                    \n",
    "                    # Reverse the transformation on prediction\n",
    "                    pred_reversed = reverse_tta_transforms(pred, transform_name)\n",
    "                    patch_predictions.append(pred_reversed)\n",
    "            \n",
    "            # Average predictions from all TTA transformations\n",
    "            avg_patch_pred = torch.stack(patch_predictions).mean(dim=0)\n",
    "            \n",
    "            # Add to prediction canvas\n",
    "            prediction_canvas[:, :, y:y+patch_size, x:x+patch_size] += avg_patch_pred\n",
    "            count_canvas[:, :, y:y+patch_size, x:x+patch_size] += 1\n",
    "            \n",
    "            # Progress update\n",
    "            if patch_count % 50 == 0 or patch_count == total_patches:\n",
    "                print(f\"Processed {patch_count}/{total_patches} patches\")\n",
    "    \n",
    "    # Average overlapping predictions (with safety check for division by zero)\n",
    "    # Add small epsilon to avoid division by zero\n",
    "    epsilon = 1e-8\n",
    "    final_prediction = prediction_canvas / (count_canvas + epsilon)\n",
    "    \n",
    "    # Verify complete coverage\n",
    "    min_count = count_canvas.min().item()\n",
    "    max_count = count_canvas.max().item()\n",
    "    print(f\"Coverage verification: min_count={min_count}, max_count={max_count}\")\n",
    "    \n",
    "    if min_count == 0:\n",
    "        print(\"⚠️  Warning: Some pixels were not covered by any patches!\")\n",
    "        # Show which areas weren't covered\n",
    "        uncovered_mask = (count_canvas[0, 0] == 0).cpu().numpy()\n",
    "        uncovered_pixels = uncovered_mask.sum()\n",
    "        print(f\"   Uncovered pixels: {uncovered_pixels}/{count_canvas.numel()}\")\n",
    "    \n",
    "    print(\"TTA inference completed!\")\n",
    "    return final_prediction\n",
    "\n",
    "\n",
    "def visualize_tta_results(original_image, ground_truth, tta_prediction, regular_prediction=None):\n",
    "    \"\"\"\n",
    "    Visualize TTA results compared to ground truth and regular prediction\n",
    "    \n",
    "    Args:\n",
    "        original_image: Original image tensor [1, 3, H, W]\n",
    "        ground_truth: Ground truth tensor [H, W] \n",
    "        tta_prediction: TTA prediction tensor [1, num_classes, H, W]\n",
    "        regular_prediction: Optional regular prediction for comparison\n",
    "    \"\"\"\n",
    "    # Convert predictions to class labels\n",
    "    tta_pred_labels = torch.argmax(tta_prediction, dim=1)[0].cpu()\n",
    "    \n",
    "    # Denormalize image for visualization\n",
    "    if original_image.dim() == 4:\n",
    "        img = original_image[0]\n",
    "    else:\n",
    "        img = original_image\n",
    "        \n",
    "    mean = torch.tensor([0.485, 0.456, 0.406])\n",
    "    std = torch.tensor([0.229, 0.224, 0.225])\n",
    "    img_denorm = img * std[:, None, None] + mean[:, None, None]\n",
    "    img_denorm = torch.clamp(img_denorm, 0, 1)\n",
    "    \n",
    "    # Create visualization\n",
    "    if regular_prediction is not None:\n",
    "        fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "        reg_pred_labels = torch.argmax(regular_prediction, dim=1)[0].cpu()\n",
    "        \n",
    "        axes[3].imshow(reg_pred_labels, cmap='tab10', vmin=0, vmax=5)\n",
    "        axes[3].set_title('Regular Prediction')\n",
    "        axes[3].axis('off')\n",
    "    else:\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    # Original image\n",
    "    axes[0].imshow(img_denorm.permute(1, 2, 0))\n",
    "    axes[0].set_title('Original Image')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Ground truth\n",
    "    axes[1].imshow(ground_truth, cmap='tab10', vmin=0, vmax=5)\n",
    "    axes[1].set_title('Ground Truth')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    # TTA prediction\n",
    "    im = axes[2].imshow(tta_pred_labels, cmap='tab10', vmin=0, vmax=5)\n",
    "    axes[2].set_title('TTA Prediction')\n",
    "    axes[2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def run_tta_evaluation(use_holdout=False, max_images=None, visualize=True):\n",
    "    \"\"\"\n",
    "    Run TTA evaluation on full-resolution test images (paper-compliant approach)\n",
    "    \n",
    "    Args:\n",
    "        use_holdout: If True, use holdout_loader instead of test_loader\n",
    "        max_images: Maximum number of images to process (None for all)\n",
    "        visualize: Whether to show visualizations for each image\n",
    "    \"\"\"\n",
    "    # Select the appropriate dataset\n",
    "    if use_holdout:\n",
    "        if 'holdout_loader' not in locals() and 'holdout_loader' not in globals():\n",
    "            print(\"Holdout loader not available. Please run the dataset loading cell first.\")\n",
    "            return\n",
    "        selected_loader = holdout_loader\n",
    "        dataset_name = \"holdout\"\n",
    "    else:\n",
    "        if 'test_loader' not in locals() and 'test_loader' not in globals():\n",
    "            print(\"Test loader not available. Please run the dataset loading cell first.\")\n",
    "            return\n",
    "        selected_loader = test_loader\n",
    "        dataset_name = \"test\"\n",
    "    \n",
    "    print(f\"Running TTA Evaluation with Full-Resolution Images ({dataset_name.upper()} SET)\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Load best model\n",
    "    if os.path.exists('best_model.pth'):\n",
    "        print(\"Loading best trained model for TTA...\")\n",
    "        trainer.load_model('best_model.pth')\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    test_dataset = selected_loader.dataset\n",
    "    \n",
    "    # Show available images\n",
    "    available_images = test_dataset.get_available_images()\n",
    "    print(f\"\\nAvailable {dataset_name} images: {len(available_images)}\")\n",
    "    \n",
    "    # Limit number of images if specified\n",
    "    if max_images is not None and max_images < len(available_images):\n",
    "        available_images = available_images[:max_images]\n",
    "        print(f\"Processing first {max_images} images for demonstration\")\n",
    "    \n",
    "    # Initialize aggregated metrics\n",
    "    all_tta_accuracies = []\n",
    "    all_reg_accuracies = []\n",
    "    all_tta_ious = []\n",
    "    all_reg_ious = []\n",
    "    all_class_tta_accuracies = [[] for _ in range(NUM_CLASSES)]\n",
    "    all_class_reg_accuracies = [[] for _ in range(NUM_CLASSES)]\n",
    "    \n",
    "    print(f\"\\nProcessing {len(available_images)} images...\")\n",
    "    \n",
    "    # Process each available image\n",
    "    for img_idx, img_name in enumerate(available_images):\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Processing Image {img_idx + 1}/{len(available_images)}: {img_name}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        # Load full-resolution image and label\n",
    "        try:\n",
    "            full_image, full_label = test_dataset.get_full_image(img_idx)\n",
    "            print(f\"Loaded full image shape: {full_image.shape}\")\n",
    "            print(f\"Loaded full label shape: {full_label.shape}\")\n",
    "            \n",
    "            # Check if image is large enough for TTA\n",
    "            _, _, height, width = full_image.shape\n",
    "            min_size_for_tta = 448 + 100  # patch_size + stride\n",
    "            \n",
    "            if height < min_size_for_tta or width < min_size_for_tta:\n",
    "                print(f\"Image size ({height}×{width}) is smaller than minimum for TTA ({min_size_for_tta}×{min_size_for_tta})\")\n",
    "                print(\"TTA will still work but with reduced patch overlap.\")\n",
    "            \n",
    "            # For very large images, we might want to crop a region for demonstration\n",
    "            if height > 2000 or width > 2000:\n",
    "                print(f\"Large image detected ({height}×{width}). Cropping to 1500×1500 for faster processing.\")\n",
    "                crop_h = min(1500, height)\n",
    "                crop_w = min(1500, width)\n",
    "                start_h = (height - crop_h) // 2\n",
    "                start_w = (width - crop_w) // 2\n",
    "                \n",
    "                full_image = full_image[:, :, start_h:start_h+crop_h, start_w:start_w+crop_w]\n",
    "                full_label = full_label[start_h:start_h+crop_h, start_w:start_w+crop_w]\n",
    "                \n",
    "                print(f\"Cropped to: {full_image.shape}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading full image {img_name}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        # Apply TTA\n",
    "        print(f\"\\nApplying Test Time Augmentation...\")\n",
    "        print(f\"Using 448×448 patches with 100-pixel stride (paper specification)\")\n",
    "        \n",
    "        try:\n",
    "            tta_prediction = test_time_augmentation_sliding_window(\n",
    "                model=model,\n",
    "                image=full_image,\n",
    "                patch_size=448,\n",
    "                stride=100,\n",
    "                device=device\n",
    "            )\n",
    "            \n",
    "            # Regular prediction for comparison (resize full image to fit model if too large)\n",
    "            print(f\"\\nRunning regular prediction for comparison...\")\n",
    "            with torch.no_grad():\n",
    "                # For regular prediction, we need to handle potentially large images\n",
    "                _, _, h, w = full_image.shape\n",
    "                max_size = 1024  # Maximum size for regular prediction to avoid memory issues\n",
    "                \n",
    "                if h > max_size or w > max_size:\n",
    "                    # Resize for regular prediction\n",
    "                    scale_factor = max_size / max(h, w)\n",
    "                    resized_image = F.interpolate(full_image, scale_factor=scale_factor, mode='bilinear', align_corners=False)\n",
    "                    regular_prediction = model(resized_image.to(device))\n",
    "                    # Resize back to original size\n",
    "                    regular_prediction = F.interpolate(regular_prediction, size=(h, w), mode='bilinear', align_corners=False)\n",
    "                else:\n",
    "                    regular_prediction = model(full_image.to(device))\n",
    "            \n",
    "            # Visualize results for this image (if enabled)\n",
    "            if visualize:\n",
    "                print(f\"\\nVisualizing results for {img_name}...\")\n",
    "                visualize_tta_results(\n",
    "                    original_image=full_image,\n",
    "                    ground_truth=full_label,\n",
    "                    tta_prediction=tta_prediction,\n",
    "                    regular_prediction=regular_prediction\n",
    "                )\n",
    "            \n",
    "            # Calculate metrics for this image\n",
    "            tta_pred_labels = torch.argmax(tta_prediction, dim=1)[0].cpu()\n",
    "            reg_pred_labels = torch.argmax(regular_prediction, dim=1)[0].cpu()\n",
    "            \n",
    "            # Overall accuracy\n",
    "            tta_acc = (tta_pred_labels == full_label).float().mean().item()\n",
    "            reg_acc = (reg_pred_labels == full_label).float().mean().item()\n",
    "            \n",
    "            all_tta_accuracies.append(tta_acc)\n",
    "            all_reg_accuracies.append(reg_acc)\n",
    "            \n",
    "            print(f\"\\nResults for {img_name}:\")\n",
    "            print(f\"Image size: {full_label.shape}\")\n",
    "            print(f\"Regular Prediction Accuracy: {reg_acc:.4f}\")\n",
    "            print(f\"TTA Prediction Accuracy: {tta_acc:.4f}\")\n",
    "            print(f\"TTA Improvement: {tta_acc - reg_acc:+.4f}\")\n",
    "            \n",
    "            # Class-wise accuracy for this image\n",
    "            print(f\"\\nClass-wise Accuracy for {img_name}:\")\n",
    "            for class_id in range(NUM_CLASSES):\n",
    "                mask = (full_label == class_id)\n",
    "                if mask.sum() > 0:\n",
    "                    tta_class_acc = (tta_pred_labels[mask] == class_id).float().mean().item()\n",
    "                    reg_class_acc = (reg_pred_labels[mask] == class_id).float().mean().item()\n",
    "                    improvement = tta_class_acc - reg_class_acc\n",
    "                    \n",
    "                    all_class_tta_accuracies[class_id].append(tta_class_acc)\n",
    "                    all_class_reg_accuracies[class_id].append(reg_class_acc)\n",
    "                    \n",
    "                    print(f\"  {CLASS_NAMES[class_id]:<20}: Regular {reg_class_acc:.3f} | TTA {tta_class_acc:.3f} | Δ{improvement:+.3f}\")\n",
    "            \n",
    "            # Calculate IoU for this image\n",
    "            def calculate_iou(pred, target, num_classes):\n",
    "                ious = []\n",
    "                for c in range(num_classes):\n",
    "                    pred_c = (pred == c)\n",
    "                    target_c = (target == c)\n",
    "                    intersection = (pred_c & target_c).sum().float()\n",
    "                    union = (pred_c | target_c).sum().float()\n",
    "                    if union > 0:\n",
    "                        ious.append((intersection / union).item())\n",
    "                    else:\n",
    "                        ious.append(0.0)\n",
    "                return ious\n",
    "            \n",
    "            tta_ious = calculate_iou(tta_pred_labels, full_label, NUM_CLASSES)\n",
    "            reg_ious = calculate_iou(reg_pred_labels, full_label, NUM_CLASSES)\n",
    "            \n",
    "            all_tta_ious.append(np.mean(tta_ious))\n",
    "            all_reg_ious.append(np.mean(reg_ious))\n",
    "            \n",
    "            print(f\"\\nMean IoU for {img_name}:\")\n",
    "            print(f\"Regular Prediction mIoU: {np.mean(reg_ious):.4f}\")\n",
    "            print(f\"TTA Prediction mIoU: {np.mean(tta_ious):.4f}\")\n",
    "            print(f\"TTA mIoU Improvement: {np.mean(tta_ious) - np.mean(reg_ious):+.4f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image {img_name}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Aggregate results across all images\n",
    "    if all_tta_accuracies:\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"AGGREGATED RESULTS ACROSS ALL {len(all_tta_accuracies)} IMAGES ({dataset_name.upper()} SET)\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        # Overall metrics\n",
    "        avg_tta_acc = np.mean(all_tta_accuracies)\n",
    "        avg_reg_acc = np.mean(all_reg_accuracies)\n",
    "        avg_tta_miou = np.mean(all_tta_ious)\n",
    "        avg_reg_miou = np.mean(all_reg_ious)\n",
    "        \n",
    "        print(f\"\\nOverall Performance:\")\n",
    "        print(f\"Average Regular Accuracy: {avg_reg_acc:.4f} (±{np.std(all_reg_accuracies):.4f})\")\n",
    "        print(f\"Average TTA Accuracy: {avg_tta_acc:.4f} (±{np.std(all_tta_accuracies):.4f})\")\n",
    "        print(f\"Average Accuracy Improvement: {avg_tta_acc - avg_reg_acc:+.4f}\")\n",
    "        \n",
    "        print(f\"\\nAverage Regular mIoU: {avg_reg_miou:.4f} (±{np.std(all_reg_ious):.4f})\")\n",
    "        print(f\"Average TTA mIoU: {avg_tta_miou:.4f} (±{np.std(all_tta_ious):.4f})\")\n",
    "        print(f\"Average mIoU Improvement: {avg_tta_miou - avg_reg_miou:+.4f}\")\n",
    "        \n",
    "        # Class-wise aggregated results\n",
    "        print(f\"\\nAggregated Class-wise Performance:\")\n",
    "        for class_id in range(NUM_CLASSES):\n",
    "            if all_class_tta_accuracies[class_id]:  # Only if this class appeared in the images\n",
    "                avg_tta_class = np.mean(all_class_tta_accuracies[class_id])\n",
    "                avg_reg_class = np.mean(all_class_reg_accuracies[class_id])\n",
    "                improvement = avg_tta_class - avg_reg_class\n",
    "                \n",
    "                print(f\"  {CLASS_NAMES[class_id]:<20}: Regular {avg_reg_class:.3f} | TTA {avg_tta_class:.3f} | Δ{improvement:+.3f}\")\n",
    "        \n",
    "        # Performance distribution plot\n",
    "        if len(all_tta_accuracies) > 1:\n",
    "            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "            \n",
    "            # Accuracy comparison\n",
    "            ax1.scatter(range(len(all_reg_accuracies)), all_reg_accuracies, alpha=0.7, label='Regular', color='blue')\n",
    "            ax1.scatter(range(len(all_tta_accuracies)), all_tta_accuracies, alpha=0.7, label='TTA', color='red')\n",
    "            ax1.plot([avg_reg_acc] * len(all_reg_accuracies), '--', color='blue', alpha=0.7, label=f'Avg Regular ({avg_reg_acc:.3f})')\n",
    "            ax1.plot([avg_tta_acc] * len(all_tta_accuracies), '--', color='red', alpha=0.7, label=f'Avg TTA ({avg_tta_acc:.3f})')\n",
    "            ax1.set_xlabel('Image Index')\n",
    "            ax1.set_ylabel('Accuracy')\n",
    "            ax1.set_title(f'Accuracy Comparison Across {len(all_tta_accuracies)} Images')\n",
    "            ax1.legend()\n",
    "            ax1.grid(True, alpha=0.3)\n",
    "            \n",
    "            # mIoU comparison\n",
    "            ax2.scatter(range(len(all_reg_ious)), all_reg_ious, alpha=0.7, label='Regular', color='blue')\n",
    "            ax2.scatter(range(len(all_tta_ious)), all_tta_ious, alpha=0.7, label='TTA', color='red')\n",
    "            ax2.plot([avg_reg_miou] * len(all_reg_ious), '--', color='blue', alpha=0.7, label=f'Avg Regular ({avg_reg_miou:.3f})')\n",
    "            ax2.plot([avg_tta_miou] * len(all_tta_ious), '--', color='red', alpha=0.7, label=f'Avg TTA ({avg_tta_miou:.3f})')\n",
    "            ax2.set_xlabel('Image Index')\n",
    "            ax2.set_ylabel('mIoU')\n",
    "            ax2.set_title(f'mIoU Comparison Across {len(all_tta_ious)} Images')\n",
    "            ax2.legend()\n",
    "            ax2.grid(True, alpha=0.3)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "    \n",
    "    else:\n",
    "        print(\"No images were successfully processed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff982ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run TTA Evaluation\n",
    "# This will demonstrate the paper's TTA approach on test samples\n",
    "\n",
    "# if 'test_loader' in locals():\n",
    "#     print(\"TTA Evaluation Options:\")\n",
    "#     print(\"1. Process all test images with visualization\")\n",
    "#     print(\"2. Process limited test images (faster)\")\n",
    "#     print(\"3. Use holdout dataset instead of test dataset\")\n",
    "#     print()\n",
    "    \n",
    "#     print(\"Running TTA on first 3 test images...\")\n",
    "#     run_tta_evaluation(use_holdout=False, max_images=3, visualize=True)\n",
    "# else:\n",
    "#     print(\"Test loader not available.\")\n",
    "#     print(\"Please run the dataset loading cells first to enable TTA evaluation.\")\n",
    "\n",
    "if 'holdout_loader' in locals():\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Running TTA on holdout dataset...\")\n",
    "    run_tta_evaluation(use_holdout=True, visualize=True)\n",
    "else:\n",
    "    print(\"\\nHoldout dataset not available (this is normal if dataset='potsdam' or 'vaihingen')\")\n",
    "    print(\"Holdout dataset is only available when dataset='both'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e469977",
   "metadata": {},
   "source": [
    "## Model Saving and Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e4039a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "import os\n",
    "\n",
    "# Create models directory if it doesn't exist\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "# Save model state dict\n",
    "model_path = 'models/ddcm_net_trained85v2tta.pth'\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'training_history': history,\n",
    "    'class_names': CLASS_NAMES\n",
    "}, model_path)\n",
    "\n",
    "print(f\"Model saved to: {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c63cab",
   "metadata": {},
   "source": [
    "### Load a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8efcef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_trained_model(model_path):\n",
    "#     \"\"\"Load a trained DDCM-Net model\"\"\"\n",
    "#     checkpoint = torch.load(model_path, map_location=device, weights_only=False)\n",
    "    \n",
    "#     # Recreate model with same config\n",
    "#     config = checkpoint['model_config']\n",
    "#     loaded_model = DDCMNet(\n",
    "#         num_classes=config['num_classes'],\n",
    "#         variant=config['variant']\n",
    "#     )\n",
    "    \n",
    "#     # Load weights\n",
    "#     loaded_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "#     loaded_model.to(device)\n",
    "#     loaded_model.eval()\n",
    "    \n",
    "#     print(f\"Model loaded from: {model_path}\")\n",
    "#     print(f\"Configuration: {config}\")\n",
    "    \n",
    "#     return loaded_model, checkpoint['training_history']\n",
    "\n",
    "# model_path = '/models/ddcm_net_trained.pth'\n",
    "# model, history = load_trained_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92bfd8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
